 <!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Advancing Weakly Supervised Multimodal Video
Anomaly Detection with Textual Guiding</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Enhancing Weakly Supervised Multimodal Video Anomaly Detection through Textual Guidance <br> with Textual Guiding</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://shengyangsun.github.io/" target="_blank">Shengyang Sun</a><sup>1</sup>,</span>
                <span class="author-block">
                  Jiashen Hua<sup>2</sup>,</span>
              <span class="author-block">
                  Junyi Feng<sup>2</sup>,</span>
                  <span class="author-block">
                    Xiaojin Gong<sup>1*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>College of Information Science & Electronic Engineering,
Zhejiang University<br><sup>2</sup>Alibaba Cloud</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Corresponding Author
</small></span>
                  </div>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code (Coming Soon)</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv (Coming Soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/TGMVAD_framework.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We propose a novel textual-guided weakly supervised multimodal video anomaly detection (TG-MVAD) framework. In detail, we introduce a multi-stage textual augmentation (MSTA) mechanism to generate high-quality anomaly text samples, counteracting training biases, and obtaining a text feature extractor that is better suited for anomaly detection. Additionally, we present a multi-scale bottleneck transformer (MSBT) fusion module to enhance multimodal integration, utilizing a set of reduced bottleneck tokens to progressively transmit compressed information across modalities.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- demo -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Demo of Anomalies Detected by TG-MVAD</h2>
        <div class="content has-text-justified">
          <p>
           Our proposed TG-MVAD introduces the text modality for anomaly detection, enhancing explainability while achieving strong performance. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End demo -->

<!-- Demo video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/TGMVAD_demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div>
</section>
<!-- End Demo video -->


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, weakly supervised multimodal video anomaly detection, which leverages RGB, optical flow, and audio modalities, has garnered significant attention from researchers, emerging as a vital subfield within video anomaly detection. However, previous studies have inadequately explored the role of textual modalities in this domain. With the proliferation of large-scale text-annotated video datasets and the advent of video captioning models, obtaining textual descriptions from videos has become increasingly feasible. Textual modalities, carrying explicit semantic information, can more accurately characterize events within videos and identify anomalies, thereby enhancing the model's detection capabilities and reducing false alarms. To investigate the impact of textual modalities on video anomaly detection, we propose a novel textual-guided weakly supervised multimodal video anomaly detection framework. Specifically, we introduce an in-context learning based multi-stage textual augmentation mechanism to generate high-quality anomaly text samples, counteracting training biases, and obtaining a text feature extractor that is better suited for anomaly detection. Additionally, we present a multi-scale bottleneck transformer fusion module to enhance multimodal integration, utilizing a set of reduced bottleneck tokens to progressively transmit compressed information across modalities. Experimental results on large-scale datasets UCF-Crime and XD-Violence demonstrate that our proposed approach achieves state-of-the-art performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

  <!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Image samples -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/imbalanced_text_samples.jpg" alt="imbalanced text samples" style="width: 70%; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
          The textual descriptions of anomalous events are often scarce and may contain ambiguities, we observed that directly applying the textual features extracted by the vanilla feature extractor to anomaly detection did not yield satisfactory performance. This observation encourages us to fine-tune a text feature extractor for the specific task of video anomaly detection. However, the scarcity of anomalous events results in a predominance of normal text samples within the video data.
  
Using the UCF-Crime dataset as a case study, we observe that only 12% of the textual samples are classified as anomalies. This imbalance in the dataset introduces bias during the feature extractor fine-tuning. To address this issue, we propose the multi-stage textual augmentation (MSTA) approach, which generates a greater number of high-quality anomaly samples to address the challenge of sample imbalance.
        </h2>
      </div>
    </div>
  </div>
</div>
</section>
<!-- End image samples -->


<!-- MSTA -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Multi-stage Textual Augmentation (MSTA)</h2>
        <div class="content has-text-justified">
          <p>
            We introduce an in-context learning (ICL) based multi- stage textual augmentation (MSTA) mechanism aimed at generating more high-quality abnormal textual samples to counteract the bias of the text feature extractor fine-tuning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End MSTA -->
      
<!-- Image MSTA -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/MSTA.jpg" alt="MSTA" style="width: 80%; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
          Illustration of the proposed multi-stage textual augmentation (MSTA). (a) Stage-I: We use an LLM to summarize all captions in the videos, obtaining labeled text samples. (b) Stage-II: based on the summarized captions, we utilize ICL to generate pseudo-labels for each caption within the video. (c) Stage-III: We employ the labeled samples from the previous two stages, using ICL to generate new anomalous samples.
        </h2>
      </div>
    </div>
  </div>
</div>
</section>
<!-- End image MSTA -->

<!-- MSBT -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">MSBT-based Multimodal Video Anomaly Detection</h2>
        <div class="content has-text-justified">
          <p>
            We propose a multi-scale bottleneck transformer (MSBT) module that improves inter-modality integration. This module employs a reduced set of bottleneck tokens to progressively convey condensed information between modalities, effectively capturing complex dependencies.The proposed MSBT-based textual-guided multimodal video anomaly detection (TG-MVAD) framework is shown below. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End MSBT -->
  

<!-- Image MSBT -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <!-- Your image here -->
        <img src="static/images/MSBT.jpg" alt="MSBT" style="width: 60%; height: auto; display: block; margin: 0 auto;"/>
        <h2 class="subtitle has-text-centered">
          An overview of the proposed framework. It includes three unimodal encoders, a multi-scale bottleneck transformer, and a global encoder for multimodal feature generation. Each unimodal encoder consists of a modality-specific feature extractor and a linear projection layer for tokenization and a modality-shared transformer for context aggregation within one modality. The multi-scale bottleneck transformer (MSBT) fuses any pair of modalities and a sub-module to weight concatenated fused features. The global encoder, implemented by a transformer, aggregates context overall snippets. Finally, the final anomaly score is constructed by combining the anomaly scores from the fused features and the anomaly probabilities from the text. 
        </h2>
      </div>
    </div>
  </div>
</div>
</section>
<!-- End image MSBT -->




<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
